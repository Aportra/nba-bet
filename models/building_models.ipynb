{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime as date\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import shap\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features= [\"min_3gm_avg\", \"fgm_3gm_avg\", \"fga_3gm_avg\", \"fg%_3gm_avg\", \"3pm_3gm_avg\", \n",
    "                   \"3pa_3gm_avg\", \"3p%_3gm_avg\", \"ftm_3gm_avg\", \"fta_3gm_avg\", \"ft%_3gm_avg\", \n",
    "                   \"oreb_3gm_avg\", \"dreb_3gm_avg\", \"reb_3gm_avg\", \"ast_3gm_avg\", \"stl_3gm_avg\", \n",
    "                   \"blk_3gm_avg\", \"to_3gm_avg\", \"pf_3gm_avg\", \"pts_3gm_avg\", \"plus_mins_3gm_avg\",\n",
    "                   \"pts_season\", \"pts_momentum\", \"min_season\", \"min_momentum\", \"fgm_season\", \"fgm_momentum\", \n",
    "                    \"fga_season\", \"fga_momentum\", \"fg%_season\", \"fg%_momentum\", \"3pm_season\", \"3pm_momentum\", \n",
    "                    \"3pa_season\", \"3pa_momentum\", \"3p%_season\", \"3p%_momentum\", \"ftm_season\", \"ftm_momentum\", \n",
    "                    \"fta_season\", \"fta_momentum\", \"ft%_season\", \"ft%_momentum\", \"oreb_season\", \"oreb_momentum\", \n",
    "                    \"dreb_season\", \"dreb_momentum\", \"reb_season\", \"reb_momentum\", \"ast_season\", \"ast_momentum\", \n",
    "                    \"stl_season\", \"stl_momentum\", \"blk_season\", \"blk_momentum\", \"to_season\", \"to_momentum\", \n",
    "                    \"pf_season\", \"pf_momentum\", \"plus_mins_season\", \"plus_mins_momentum\"]\n",
    "\n",
    "team_features = [\"home\",\"away\",\"offrtg_3gm_avg\", \"defrtg_3gm_avg\", \"netrtg_3gm_avg\", \"ast%_3gm_avg\", \"ast_to_3gm_avg\", \n",
    "                    \"ast_ratio_3gm_avg\", \"oreb%_3gm_avg\", \"dreb%_3gm_avg\", \"reb%_3gm_avg\", \"tov%_3gm_avg\", \n",
    "                    \"efg%_3gm_avg\", \"ts%_3gm_avg\", \"pace_3gm_avg\", \"pie_3gm_avg\",\n",
    "                    \"netrtg_season\", \"netrtg_momentum\", \"offrtg_season\", \"offrtg_momentum\", \"defrtg_season\", \"defrtg_momentum\", \n",
    "                    \"ast%_season\", \"ast%_momentum\", \"ast_to_season\", \"ast_to_momentum\", \"ast_ratio_season\", \"ast_ratio_momentum\", \n",
    "                    \"oreb%_season\", \"oreb%_momentum\", \"dreb%_season\", \"dreb%_momentum\", \"reb%_season\", \"reb%_momentum\", \n",
    "                    \"tov%_season\", \"tov%_momentum\", \"efg%_season\", \"efg%_momentum\", \"ts%_season\", \"ts%_momentum\", \n",
    "                    \"pace_season\", \"pace_momentum\", \"pie_season\", \"pie_momentum\"]\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT player,team,game_id,game_date,matchup,pts,reb,ast,`3pm`, {','.join([f'`{player}`' for player in player_features])},season\n",
    "from `capstone_data.player_modeling_data`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT team,game_id,game_date,home,away, {', '.join([f'`{team}`' for team in team_features])}\n",
    "from `capstone_data.team_modeling_data`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203'))\n",
    "    opponent_data = team_data.rename(columns={\n",
    "    col: ('matchup' if col == 'team' else 'game_id' if col == 'game_id' else f'opponent_{col}') for col in team_data.columns})\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data = full_data.merge(opponent_data,on = ['game_id','matchup'],how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['pts_per_min_3gm'] = data_ordered['pts_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['pts_per_min_season'] = data_ordered['pts_season']/data_ordered['min_season']\n",
    "data_ordered['pts_per_min_momentum'] = data_ordered['pts_per_min_3gm'] - data_ordered['pts_per_min_season']\n",
    "\n",
    "data_ordered['3pm_per_min_3gm'] = data_ordered['3pm_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['3pm_per_min_season'] = data_ordered['3pm_season']/data_ordered['min_season']\n",
    "data_ordered['3pm_per_min_momentum'] = data_ordered['3pm_per_min_3gm'] - data_ordered['3pm_per_min_season'] \n",
    "\n",
    "data_ordered['reb_per_min_3gm'] = data_ordered['reb_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['reb_per_min_season'] = data_ordered['reb_season']/data_ordered['min_season']\n",
    "data_ordered['reb_per_min_momentum'] = data_ordered['3pm_per_min_3gm'] - data_ordered['reb_per_min_season']\n",
    "\n",
    "home_performance = data_ordered[data_ordered['home'] == 1]\n",
    "away_performance = data_ordered[data_ordered['away'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is sorted correctly for chronological calculations\n",
    "data_ordered = data_ordered.sort_values(by=['player', 'season', 'game_date'])\n",
    "\n",
    "# Separate home and away games\n",
    "home_performance = data_ordered[data_ordered['home'] == 1]\n",
    "away_performance = data_ordered[data_ordered['home'] == 0]  # Fixed to align with `home` flag\n",
    "\n",
    "# Compute season-to-date averages for home and away games (including game_id)\n",
    "home_rolling = (\n",
    "    home_performance.groupby(['player', 'season'])[['game_id', 'pts', 'reb', 'ast', '3pm']]\n",
    "    .apply(lambda x: x.set_index('game_id').expanding().mean().shift(1))  # Prevent data leakage\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "away_rolling = (\n",
    "    away_performance.groupby(['player', 'season'])[['game_id', 'pts', 'reb', 'ast', '3pm']]\n",
    "    .apply(lambda x: x.set_index('game_id').expanding().mean().shift(1))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns before merging\n",
    "home_rolling = home_rolling.rename(columns={'pts': 'home_avg_pts', 'reb': 'home_avg_reb', \n",
    "                                            'ast': 'home_avg_ast', '3pm': 'home_avg_3pm'})\n",
    "away_rolling = away_rolling.rename(columns={'pts': 'away_avg_pts', 'reb': 'away_avg_reb', \n",
    "                                            'ast': 'away_avg_ast', '3pm': 'away_avg_3pm'})\n",
    "\n",
    "# Merge rolling averages back into `data_ordered`\n",
    "data_ordered = data_ordered.merge(home_rolling[['player', 'game_id', 'home_avg_pts', 'home_avg_reb', 'home_avg_ast', 'home_avg_3pm']],\n",
    "                                  on=['player', 'game_id'], how='left')\n",
    "\n",
    "data_ordered = data_ordered.merge(away_rolling[['player', 'game_id', 'away_avg_pts', 'away_avg_reb', 'away_avg_ast', 'away_avg_3pm']],\n",
    "                                  on=['player', 'game_id'], how='left')\n",
    "\n",
    "# Fill missing values for early season games\n",
    "for cat in ['pts', 'reb', 'ast', '3pm']:\n",
    "    data_ordered[f'home_avg_{cat}'] = data_ordered[f'home_avg_{cat}'].fillna(0)\n",
    "    data_ordered[f'away_avg_{cat}'] = data_ordered[f'away_avg_{cat}'].fillna(0)\n",
    "\n",
    "    # Compute home vs. away performance difference conditionally\n",
    "    data_ordered[f'{cat}_home_away_diff'] = (\n",
    "        (data_ordered['home'] == 1) * (data_ordered[f'home_avg_{cat}'] - data_ordered[f'away_avg_{cat}']) +\n",
    "        (data_ordered['home'] == 0) * (data_ordered[f'away_avg_{cat}'] - data_ordered[f'home_avg_{cat}'])\n",
    "    )\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data_ordered = data_ordered.drop(columns=[f'home_avg_{cat}' for cat in ['pts', 'reb', 'ast', '3pm']] + \n",
    "                                           [f'away_avg_{cat}' for cat in ['pts', 'reb', 'ast', '3pm']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.drop(columns = ['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL APPLY POLYNOMIAL FEATURES IN THE FUTURE\n",
    "# poly = PolynomialFeatures(2,include_bias=False,interaction_only=False)\n",
    "# percentages = [feature for feature in data_ordered.columns if '%' in feature]\n",
    "\n",
    "# for col in percentages:\n",
    "#     transformed_data = poly.fit_transform(data_ordered[[col]])\n",
    "\n",
    "#     column_names = [f\"{col}_poly_{i+1}\" for i in range(transformed_data.shape[1])]\n",
    "\n",
    "#     poly_df = pd.DataFrame(transformed_data,columns=column_names,index=data_ordered)\n",
    "\n",
    "# print(poly_df['opponent_ts%_momentum_poly_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight']]\n",
    "\n",
    "features = {feature:[] for feature in ['pts','reb','ast','3pm']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    print(category)\n",
    "    for column in numeric_columns:\n",
    "        correlation = pearsonr(data_ordered[column],data_ordered[category])\n",
    "        if correlation[1] < .05 and abs(correlation[0]) > .3:\n",
    "            print(column)\n",
    "            print(f'correlation {correlation[0]} p_value {correlation[1]}')\n",
    "            features[category].append(column)\n",
    "    features[category].append('time_decay_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking spearmanr \n",
    "\n",
    "for category in features.keys():\n",
    "    print(category)\n",
    "    for column in numeric_columns:\n",
    "        pearson_corr, pearson_p = pearsonr(data_ordered[column], data_ordered[category])\n",
    "        spearman_corr, spearman_p = spearmanr(data_ordered[column], data_ordered[category])\n",
    "\n",
    "        # If Spearman is high but Pearson is low, it suggests a non-linear relationship\n",
    "        if abs(spearman_corr) > 0.3 and abs(pearson_corr) < 0.2:\n",
    "            print(f\"ðŸš€ {column} likely has a non-linear relationship with {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These values appeared to have non-linear relationships applying transformations\n",
    "data_ordered['ft%_season'] = np.log1p(data_ordered['ft%_season'])\n",
    "data_ordered['stl_3gm_avg'] = np.log1p(data_ordered['stl_3gm_avg'])\n",
    "data_ordered['stl_season'] = np.log1p(data_ordered['stl_season'])\n",
    "data_ordered['to_season'] = data_ordered['to_season']**2 \n",
    "data_ordered['to_3gm_avg'] = data_ordered['to_3gm_avg']**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {category:{} for category in features.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features[category] = [f for f in features[category] if f != category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    linear_model = LinearRegression()\n",
    "\n",
    "    linear_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':linear_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))\n",
    "\n",
    "    saved_models[category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing for multicollinearity\n",
    "mulitcol_pairs = {cat:[] for cat in features.keys()}\n",
    "for category in features.keys():\n",
    "    corr_matrix = data_ordered[features[category]].corr()\n",
    "    high_corr_vars = np.where(abs(corr_matrix) > 0.8)\n",
    "    high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                    for x, y in zip(*high_corr_vars) if x != y and x < y]\n",
    "    mulitcol_pairs[category].append(high_corr_pairs)\n",
    "      # Drop one from each highly correlated pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_feats = {cat:[] for cat in features.keys()}\n",
    "for cat in mulitcol_pairs.keys():\n",
    "    print(cat)\n",
    "    for pairs in mulitcol_pairs[cat]:\n",
    "        for x,y in pairs:\n",
    "            cor_x = pearsonr(data_ordered[x],data_ordered[cat])\n",
    "            cor_y = pearsonr(data_ordered[y],data_ordered[cat])\n",
    "\n",
    "            if cor_x[1] < .05 and cor_x[0] > cor_y[0]:\n",
    "                trimmed_feats[cat].append(x)\n",
    "            elif cor_y[1] < .05 and cor_y[0] > cor_x[0]:\n",
    "                trimmed_feats[cat].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "numeric_columns.append('time_decay_weight')\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forrest = RandomForestRegressor(n_estimators=100,criterion='squared_error',max_depth=10, min_samples_split=4,n_jobs=-1)\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = scaled_train_data[features_list],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[features_list],test_data[category]\n",
    "\n",
    "    rand_forrest.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = rand_forrest.predict(x_test)\n",
    "\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    print(category)\n",
    "    print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forrest = RandomForestRegressor(n_estimators=100,criterion='squared_error',max_depth=10, min_samples_split=4,n_jobs=-1)\n",
    "\n",
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "\n",
    "    rand_forrest.fit(x_train,y_train)\n",
    "\n",
    "    y_pred = rand_forrest.predict(x_test)\n",
    "\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    print(category)\n",
    "    print(r2)\n",
    "    saved_models[category]['Random_Forrest'] = rand_forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(data_ordered['time_decay_weight'], bins=30)\n",
    "plt.title(\"Distribution of Time Decay Weights\")\n",
    "plt.xlabel(\"Decay Weight\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,6,9],'learning_rate':[.01,.05,.1,.3],'booster':['gbtree','dart'],'subsample':[.5,.7,.9],'colsample_bytree':[.5,.7,.9],'n_estimators':[100,300,500]}\n",
    "param_linear = {'booster':['gblinear'],'lambda':[0,.1,1,10],'alpha':[0,.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgboost.XGBRegressor()\n",
    "mse_score = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "scoring = {'MSE':mse_score,'r2':r2_scorer}\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_grid,scoring = scoring,cv=tscv,n_jobs=1,verbose=0,refit='r2')\n",
    "grid_linear_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_linear,scoring = scoring,cv=tscv,n_jobs=3,verbose=0,refit='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features = [feature for feature in data_ordered.columns if data_ordered[feature].dtype == 'float' and feature not in features.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    x_train,y_train = scaled_train_data[xg_features],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[xg_features],test_data[category]\n",
    "\n",
    "    fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "    grid_linear_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "    grid_linear_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    print(category)\n",
    "    print(grid_linear_search.best_params_)\n",
    "    print(grid_linear_search.best_score_)\n",
    "\n",
    "    saved_models[category]['XGboost'] = grid_linear_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for category in features.keys():\n",
    "#     x_train,y_train = scaled_train_data[xg_features],train_data[category]\n",
    "#     x_test, y_test = scaled_test_data[xg_features],test_data[category]\n",
    "\n",
    "#     fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "#     grid_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "#     grid_search.fit(x_train,y_train)\n",
    "\n",
    "#     print(category)\n",
    "#     print(grid_search.best_params_)\n",
    "#     print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = lightgbm.LGBMRegressor(boosting_type='gbdt', n_estimators=500)  # Using hist for faster training\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],  \n",
    "    'learning_rate': [0.01, 0.1],  \n",
    "    'max_depth': [-1, 10],  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_grid_search = GridSearchCV(estimator=light,param_grid=param_grid,cv=tscv,verbose=0,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    light_grid_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = light_grid_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", light_grid_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_nn = {'activation':['identity','logistic','tanh','relu'],'solver':['lbfgs','sgd','adam'],'alpha':[.0001,.001,.01,.0005],'batch_size':[200,400,600],'learning_rate':['constant','invscaling','adaptive'],'max_iter':[500,1000]}\n",
    "\n",
    "# nn_grid = GridSearchCV(estimator=MLPRegressor(),param_grid=param_nn,scoring=scoring,cv=tscv,n_jobs=-1,verbose=0,refit='r2')\n",
    "nn_grid = RandomizedSearchCV(estimator=MLPRegressor(),param_distributions=param_nn,n_iter=30,scoring=scoring,cv=tscv,n_jobs=-1,verbose=1,refit='r2')\n",
    "\n",
    "for category in features.keys():\n",
    "    nn_features = [col for col in scaled_data_df.columns if col not in ['game_id','player','team','matchup','pts','reb','ast','3pm']]\n",
    "    scaled_x_train,scaled_y_train = scaled_train_data[nn_features],train_data[category]\n",
    "    scaled_x_test,scaled_y_test = scaled_test_data[nn_features],test_data[category]\n",
    "\n",
    "    nn_grid.fit(scaled_x_train,scaled_y_train)\n",
    "\n",
    "    best_model = nn_grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(scaled_x_test)\n",
    "\n",
    "    mse = mean_squared_error(scaled_y_test,y_pred)\n",
    "    r2 = r2_score(scaled_y_test,y_pred)\n",
    "    print(category)\n",
    "    print(r2)\n",
    "    saved_models[category]['MLP'] = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=46.84 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=554657.603, Time=0.89 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=534828.508, Time=1.96 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=33.96 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=554655.603, Time=0.41 sec\n",
      " ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=526527.360, Time=2.76 sec\n",
      " ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=521995.217, Time=3.47 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0] intercept   : AIC=519047.667, Time=4.40 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0] intercept   : AIC=517275.259, Time=5.43 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=82.87 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=62.76 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0]             : AIC=517273.259, Time=1.76 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0]             : AIC=519045.667, Time=1.61 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0]             : AIC=inf, Time=17.17 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0]             : AIC=inf, Time=15.23 sec\n",
      "\n",
      "Best model:  ARIMA(5,1,0)(0,0,0)[0]          \n",
      "Total fit time: 281.576 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts0.5414376988436713\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=44.89 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=422217.487, Time=0.94 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=401842.004, Time=1.14 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=35.02 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=422215.487, Time=0.43 sec\n",
      " ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=393627.980, Time=2.89 sec\n",
      " ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=389081.118, Time=3.48 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0] intercept   : AIC=386329.588, Time=4.26 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0] intercept   : AIC=384368.420, Time=5.36 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=87.39 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=82.50 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0]             : AIC=384366.420, Time=1.72 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0]             : AIC=386327.587, Time=1.67 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0]             : AIC=inf, Time=18.04 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0]             : AIC=inf, Time=12.66 sec\n",
      "\n",
      "Best model:  ARIMA(5,1,0)(0,0,0)[0]          \n",
      "Total fit time: 302.423 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reb-0.4917242367418173\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=46.65 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=384700.960, Time=0.92 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=364636.402, Time=1.12 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=24.33 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=384698.960, Time=0.43 sec\n",
      " ARIMA(2,1,0)(0,0,0)[0] intercept   : AIC=356224.290, Time=2.78 sec\n",
      " ARIMA(3,1,0)(0,0,0)[0] intercept   : AIC=351679.734, Time=3.40 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0] intercept   : AIC=348879.295, Time=4.29 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0] intercept   : AIC=346949.348, Time=5.49 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=84.82 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=63.88 sec\n",
      " ARIMA(5,1,0)(0,0,0)[0]             : AIC=346947.348, Time=1.91 sec\n",
      " ARIMA(4,1,0)(0,0,0)[0]             : AIC=348877.295, Time=1.79 sec\n",
      " ARIMA(5,1,1)(0,0,0)[0]             : AIC=inf, Time=15.38 sec\n",
      " ARIMA(4,1,1)(0,0,0)[0]             : AIC=inf, Time=11.54 sec\n",
      "\n",
      "Best model:  ARIMA(5,1,0)(0,0,0)[0]          \n",
      "Total fit time: 268.744 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ast0.22619320767635187\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(2,0,2)(0,0,0)[0]             : AIC=inf, Time=9.92 sec\n",
      " ARIMA(0,0,0)(0,0,0)[0]             : AIC=291231.651, Time=0.36 sec\n",
      " ARIMA(1,0,0)(0,0,0)[0]             : AIC=279533.810, Time=0.72 sec\n",
      " ARIMA(0,0,1)(0,0,0)[0]             : AIC=283717.265, Time=1.00 sec\n",
      " ARIMA(2,0,0)(0,0,0)[0]             : AIC=273768.918, Time=1.05 sec\n",
      " ARIMA(3,0,0)(0,0,0)[0]             : AIC=269939.832, Time=1.14 sec\n",
      " ARIMA(4,0,0)(0,0,0)[0]             : AIC=267733.126, Time=1.45 sec\n",
      " ARIMA(5,0,0)(0,0,0)[0]             : AIC=266121.719, Time=1.94 sec\n",
      " ARIMA(5,0,1)(0,0,0)[0]             : AIC=inf, Time=29.43 sec\n",
      " ARIMA(4,0,1)(0,0,0)[0]             : AIC=inf, Time=22.27 sec\n",
      " ARIMA(5,0,0)(0,0,0)[0] intercept   : AIC=256179.219, Time=6.13 sec\n",
      " ARIMA(4,0,0)(0,0,0)[0] intercept   : AIC=256177.311, Time=4.93 sec\n",
      " ARIMA(3,0,0)(0,0,0)[0] intercept   : AIC=256175.720, Time=3.23 sec\n",
      " ARIMA(2,0,0)(0,0,0)[0] intercept   : AIC=256183.000, Time=2.60 sec\n",
      " ARIMA(3,0,1)(0,0,0)[0] intercept   : AIC=256177.724, Time=6.26 sec\n",
      " ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=256185.121, Time=7.12 sec\n",
      " ARIMA(4,0,1)(0,0,0)[0] intercept   : AIC=256179.310, Time=9.95 sec\n",
      "\n",
      "Best model:  ARIMA(3,0,0)(0,0,0)[0] intercept\n",
      "Total fit time: 109.542 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/aportra99/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3pm0.337992633521766\n"
     ]
    }
   ],
   "source": [
    "# data_ordered = data_ordered.set_index('game_date')  # Now modifying original\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered.iloc[split_index:]\n",
    "\n",
    "data_ordered_daily = data_ordered.resample('D').agg({\n",
    "    'pts': 'sum',  \n",
    "    'reb': 'sum',  \n",
    "    'ast': 'sum',  \n",
    "    '3pm': 'sum',  \n",
    "    'game_id': 'count',  \n",
    "    'player': lambda x: list(x),  \n",
    "    'team': lambda x: list(x),  \n",
    "    'matchup': lambda x: list(x)\n",
    "})\n",
    "\n",
    "\n",
    "for category in features.keys():\n",
    "    # Aggregate statistics (Example: Total points scored in each game)\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "\n",
    "    auto_model = auto_arima(y_train,seasonal=False,trace=True,suppress_warnings=True,max_d=1,max_p=5,max_q=5)\n",
    "    p,d,q = auto_model.order\n",
    "\n",
    "    sarimax = SARIMAX(y_train,exog=x_train,order=(p,0,q),seasonal_order=(1,0,1,7))\n",
    "\n",
    "    fitted_model=sarimax.fit()\n",
    "\n",
    "    y_pred = fitted_model.forecast(steps=len(y_test),exog=x_test)\n",
    "\n",
    "    print(f'{category}{r2_score(y_test,y_pred)}')\n",
    "    saved_models[category]['sarimax'] = {\n",
    "        'order': (p, d, q),\n",
    "        'seasonal_order': (1, 0, 1, 7),\n",
    "        'params': fitted_model.params,\n",
    "        'exog_columns': list(features_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models.pkl']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(saved_models,'models.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
