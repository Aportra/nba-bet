{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime as date\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit \n",
    "from sklearn.metrics import r2_score,make_scorer,mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features= [\"min_3gm_avg\", \"fgm_3gm_avg\", \"fga_3gm_avg\", \"fg%_3gm_avg\", \"3pm_3gm_avg\", \n",
    "                   \"3pa_3gm_avg\", \"3p%_3gm_avg\", \"ftm_3gm_avg\", \"fta_3gm_avg\", \"ft%_3gm_avg\", \n",
    "                   \"oreb_3gm_avg\", \"dreb_3gm_avg\", \"reb_3gm_avg\", \"ast_3gm_avg\", \"stl_3gm_avg\", \n",
    "                   \"blk_3gm_avg\", \"to_3gm_avg\", \"pf_3gm_avg\", \"pts_3gm_avg\", \"plus_mins_3gm_avg\",\n",
    "                   \"pts_season\", \"pts_momentum\", \"min_season\", \"min_momentum\", \"fgm_season\", \"fgm_momentum\", \n",
    "                    \"fga_season\", \"fga_momentum\", \"fg%_season\", \"fg%_momentum\", \"3pm_season\", \"3pm_momentum\", \n",
    "                    \"3pa_season\", \"3pa_momentum\", \"3p%_season\", \"3p%_momentum\", \"ftm_season\", \"ftm_momentum\", \n",
    "                    \"fta_season\", \"fta_momentum\", \"ft%_season\", \"ft%_momentum\", \"oreb_season\", \"oreb_momentum\", \n",
    "                    \"dreb_season\", \"dreb_momentum\", \"reb_season\", \"reb_momentum\", \"ast_season\", \"ast_momentum\", \n",
    "                    \"stl_season\", \"stl_momentum\", \"blk_season\", \"blk_momentum\", \"to_season\", \"to_momentum\", \n",
    "                    \"pf_season\", \"pf_momentum\", \"plus_mins_season\", \"plus_mins_momentum\"]\n",
    "\n",
    "team_features = [\"home\",\"away\",\"offrtg_3gm_avg\", \"defrtg_3gm_avg\", \"netrtg_3gm_avg\", \"ast%_3gm_avg\", \"ast_to_3gm_avg\", \n",
    "                    \"ast_ratio_3gm_avg\", \"oreb%_3gm_avg\", \"dreb%_3gm_avg\", \"reb%_3gm_avg\", \"tov%_3gm_avg\", \n",
    "                    \"efg%_3gm_avg\", \"ts%_3gm_avg\", \"pace_3gm_avg\", \"pie_3gm_avg\",\n",
    "                    \"netrtg_season\", \"netrtg_momentum\", \"offrtg_season\", \"offrtg_momentum\", \"defrtg_season\", \"defrtg_momentum\", \n",
    "                    \"ast%_season\", \"ast%_momentum\", \"ast_to_season\", \"ast_to_momentum\", \"ast_ratio_season\", \"ast_ratio_momentum\", \n",
    "                    \"oreb%_season\", \"oreb%_momentum\", \"dreb%_season\", \"dreb%_momentum\", \"reb%_season\", \"reb%_momentum\", \n",
    "                    \"tov%_season\", \"tov%_momentum\", \"efg%_season\", \"efg%_momentum\", \"ts%_season\", \"ts%_momentum\", \n",
    "                    \"pace_season\", \"pace_momentum\", \"pie_season\", \"pie_momentum\"]\n",
    "\n",
    "#using shifted windows for rolling data to prevent data leakage\n",
    "player_query = f\"\"\" \n",
    "SELECT player,team,game_id,game_date,matchup,pts,reb,ast,`3pm`, {','.join([f'`{player}`' for player in player_features])},season\n",
    "from `capstone_data.player_modeling_data`\n",
    "order by game_date asc\n",
    "\"\"\"\n",
    "\n",
    "team_query = f\"\"\"\n",
    "SELECT team,game_id,game_date,home,away, {', '.join([f'`{team}`' for team in team_features])}\n",
    "from `capstone_data.team_modeling_data`\n",
    "order by game_date asc\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    full_data = pd.read_csv('full_data.csv')\n",
    "\n",
    "except:\n",
    "    nba_player_data = pd.DataFrame(pandas_gbq.read_gbq(player_query,project_id='miscellaneous-projects-444203'))\n",
    "    team_data = pd.DataFrame(pandas_gbq.read_gbq(team_query,project_id='miscellaneous-projects-444203'))\n",
    "    opponent_data = team_data.rename(columns={\n",
    "    col: ('matchup' if col == 'team' else 'game_id' if col == 'game_id' else f'opponent_{col}') for col in team_data.columns})\n",
    "    full_data = nba_player_data.merge(team_data, on = ['game_id','team'], how = 'inner',suffixes=('','remove'))\n",
    "    full_data = full_data.merge(opponent_data,on = ['game_id','matchup'],how = 'inner',suffixes=('','remove'))\n",
    "    full_data.drop([column for column in full_data.columns if 'remove' in column],axis = 1 , inplace=True) \n",
    "    full_data.drop([column for column in full_data.columns if '_1' in column],axis = 1 , inplace=True)\n",
    "    full_data.to_csv('full_data.csv',mode = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = full_data.sort_values('game_date')\n",
    "\n",
    "data_ordered.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Ideas \n",
    "\n",
    "* (ratio of 3pa and fga and 3pm and 3pa) TS% for players efg% \n",
    "* for players assist_to_turnover ratio assist ratio, \n",
    "* rebound_cahnce, defesnive reb %, \n",
    "* ast_ratio_season * pace, \n",
    "* home * pts season - data pts 3pm avg,\n",
    "* cold_streak pts_3gm_avg < pts_season boolean, \n",
    "* away difficulty away * opponent_defrtg_3gm_avg,\n",
    "* home_performance = data_ordered[data_ordered[\"home\"] == 1].groupby(\"team\")[\"pts_season\"].mean()\n",
    "* away_performance = data_ordered[data_ordered[\"away\"] == 1].groupby(\"team\")[\"pts_season\"].mean() these would be to see how the team performance changes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['pts_per_min_3gm'] = data_ordered['pts_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['pts_per_min_season'] = data_ordered['pts_season']/data_ordered['min_season']\n",
    "data_ordered['pts_per_min_momentum'] = data_ordered['pts_per_min_3gm'] - data_ordered['pts_per_min_season']\n",
    "\n",
    "data_ordered['3pm_per_min_3gm'] = data_ordered['3pm_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['3pm_per_min_season'] = data_ordered['3pm_season']/data_ordered['min_season']\n",
    "data_ordered['3pm_per_min_momentum'] = data_ordered['3pm_per_min_3gm'] - data_ordered['3pm_per_min_season'] \n",
    "\n",
    "data_ordered['reb_per_min_3gm'] = data_ordered['reb_3gm_avg']/data_ordered['min_3gm_avg']\n",
    "data_ordered['reb_per_min_season'] = data_ordered['reb_season']/data_ordered['min_season']\n",
    "data_ordered['reb_per_min_momentum'] = data_ordered['3pm_per_min_3gm'] - data_ordered['reb_per_min_season']\n",
    "\n",
    "home_performance = data_ordered[data_ordered['home'] == 1]\n",
    "away_performance = data_ordered[data_ordered['away'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure data is sorted correctly for chronological calculations\n",
    "# data_ordered = data_ordered.sort_values(by=['player', 'season', 'game_date'])\n",
    "\n",
    "# # Separate home and away games\n",
    "# home_performance = data_ordered[data_ordered['home'] == 1]\n",
    "# away_performance = data_ordered[data_ordered['home'] == 0]  # Fixed to align with `home` flag\n",
    "\n",
    "# # Compute season-to-date averages for home and away games (including game_id)\n",
    "# home_rolling = (\n",
    "#     home_performance.groupby(['player', 'season'])[['game_id', 'pts', 'reb', 'ast', '3pm']]\n",
    "#     .apply(lambda x: x.set_index('game_id').expanding().mean().shift(1))  # Prevent data leakage\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# away_rolling = (\n",
    "#     away_performance.groupby(['player', 'season'])[['game_id', 'pts', 'reb', 'ast', '3pm']]\n",
    "#     .apply(lambda x: x.set_index('game_id').expanding().mean().shift(1))\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# # Rename columns before merging\n",
    "# home_rolling = home_rolling.rename(columns={'pts': 'home_avg_pts', 'reb': 'home_avg_reb', \n",
    "#                                             'ast': 'home_avg_ast', '3pm': 'home_avg_3pm'})\n",
    "# away_rolling = away_rolling.rename(columns={'pts': 'away_avg_pts', 'reb': 'away_avg_reb', \n",
    "#                                             'ast': 'away_avg_ast', '3pm': 'away_avg_3pm'})\n",
    "\n",
    "# # Merge rolling averages back into `data_ordered`\n",
    "# data_ordered = data_ordered.merge(home_rolling[['player', 'game_id', 'home_avg_pts', 'home_avg_reb', 'home_avg_ast', 'home_avg_3pm']],\n",
    "#                                   on=['player', 'game_id'], how='left')\n",
    "\n",
    "# data_ordered = data_ordered.merge(away_rolling[['player', 'game_id', 'away_avg_pts', 'away_avg_reb', 'away_avg_ast', 'away_avg_3pm']],\n",
    "#                                   on=['player', 'game_id'], how='left')\n",
    "\n",
    "# # Fill missing values for early season games\n",
    "# for cat in ['pts', 'reb', 'ast', '3pm']:\n",
    "#     data_ordered[f'home_avg_{cat}'] = data_ordered[f'home_avg_{cat}'].fillna(0)\n",
    "#     data_ordered[f'away_avg_{cat}'] = data_ordered[f'away_avg_{cat}'].fillna(0)\n",
    "\n",
    "#     # Compute home vs. away performance difference conditionally\n",
    "#     data_ordered[f'{cat}_home_away_diff'] = (\n",
    "#         (data_ordered['home'] == 1) * (data_ordered[f'home_avg_{cat}'] - data_ordered[f'away_avg_{cat}']) +\n",
    "#         (data_ordered['home'] == 0) * (data_ordered[f'away_avg_{cat}'] - data_ordered[f'home_avg_{cat}'])\n",
    "#     )\n",
    "\n",
    "# # Drop unnecessary columns\n",
    "# data_ordered = data_ordered.drop(columns=[f'home_avg_{cat}' for cat in ['pts', 'reb', 'ast', '3pm']] + \n",
    "#                                            [f'away_avg_{cat}' for cat in ['pts', 'reb', 'ast', '3pm']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = data_ordered.groupby(['player','season']).apply(lambda x: x.iloc[3:]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered.sort_values(by='game_date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['game_date'] = pd.to_datetime(data_ordered['game_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered['days_ago'] = (data_ordered['game_date'].max() - data_ordered['game_date']).dt.days\n",
    "data_ordered['time_decay_weight'] = 1 / (1 + np.log(1 + data_ordered['days_ago']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ordered = data_ordered.drop('Unnamed: 0', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = data_ordered[data_ordered['season']=='2024-2025']['player'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_ordered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_columns = [column for column in numeric_columns if column not in ['pts','reb','ast','blk','stl','3pm','game_id','game_date','days_ago','time_decay_weight']]\n",
    "\n",
    "player_features = {player:{feature:[] for feature in ['pts','reb','ast','3pm']}for player in players}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for player in player_features.keys():\n",
    "    for category in player_features[player].keys():\n",
    "        player_data = data_ordered[data_ordered['player'] == player]\n",
    "\n",
    "        if len(player_data) > 82:  # Use player-specific data\n",
    "            x = player_data[numeric_columns]\n",
    "            y = player_data[category]\n",
    "        else:  # Use global data\n",
    "            x = data_ordered[numeric_columns]\n",
    "            y = data_ordered[category]\n",
    "\n",
    "        mi_scores = mutual_info_regression(x, y)\n",
    "        mi_scores = pd.Series(mi_scores, index=numeric_columns)\n",
    "        selected_features = mi_scores[mi_scores > 0.05].index.tolist()  # Keep features with MI > 0.05\n",
    "\n",
    "        player_features[player][category] = selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in player_features.keys():\n",
    "    for category in player_features[player].keys():\n",
    "        print(category)\n",
    "        for column in numeric_columns:\n",
    "                player_data = data_ordered[data_ordered['player'] == player]\n",
    "                if len(player_data) > 82 : \n",
    "                    pearson_corr, pearson_p = pearsonr(player_data[column], player_data[category])\n",
    "                    spearman_corr, spearman_p = spearmanr(player_data[column], player_data[category])\n",
    "                    \n",
    "                    if abs(spearman_corr) > 0.3 and abs(pearson_corr) < 0.2:\n",
    "                        print(f\"ðŸš€ {column} likely has a non-linear relationship with {category}\")\n",
    "                else: \n",
    "                    pearson_corr, pearson_p = pearsonr(data_ordered[column], data_ordered[category])\n",
    "                    spearman_corr, spearman_p = spearmanr(data_ordered[column], data_ordered[category])\n",
    "\n",
    "                    if abs(spearman_corr) > 0.3 and abs(pearson_corr) < 0.2:\n",
    "                        print(f\"ðŸš€ {column} likely has a non-linear relationship with {category}\")\n",
    "                # If Spearman is high but Pearson is low, it suggests a non-linear relationship\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These values appeared to have non-linear relationships applying transformations\n",
    "data_ordered['ft%_season'] = np.log1p(data_ordered['ft%_season'])\n",
    "data_ordered['stl_3gm_avg'] = np.log1p(data_ordered['stl_3gm_avg'])\n",
    "data_ordered['stl_season'] = np.log1p(data_ordered['stl_season'])\n",
    "data_ordered['to_season'] = data_ordered['to_season']**2 \n",
    "data_ordered['to_3gm_avg'] = data_ordered['to_3gm_avg']**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = {player:{category:{} for category in ['pts','reb','ast','3pm']} for player in players}\n",
    "saved_results = {player:{category:{} for category in ['pts','reb','ast','3pm']}for player in players}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in player_features['Damian Lillard']['ast']:\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP\n",
    "Applying shap to help reduce collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in player_features.keys():\n",
    "    for category in player_features[player].keys():\n",
    "        player_features[player][category] = [f for f in player_features[player][category] if f != category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for player in player_features.keys():\n",
    "    player_data = data_ordered[data_ordered['player'] == player]\n",
    "    if len(player_data) > 82:\n",
    "        print('enough data for player sepcific model')\n",
    "        player_data = player_data.sort_values(by='game_date')\n",
    "        split_index = int(len(player_data) * .80)\n",
    "\n",
    "        train_data = player_data.iloc[:split_index]\n",
    "        test_data = player_data[split_index:]\n",
    "\n",
    "    else:\n",
    "        split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "        train_data = data_ordered.iloc[:split_index]\n",
    "        test_data = data_ordered[split_index:]\n",
    "\n",
    "    for category in player_features[player].keys():\n",
    "        if len(player_features[player][category]) == 0:\n",
    "            continue\n",
    "        features_list = [f for f in player_features[player][category] if f != category]\n",
    "        print(len(features_list))\n",
    "        x_train,y_train = train_data[features_list],train_data[category]\n",
    "        x_test, y_test = test_data[features_list],test_data[category]\n",
    "        linear_model = LinearRegression()\n",
    "\n",
    "        linear_model.fit(x_train,y_train)\n",
    "\n",
    "        y_pred = linear_model.predict(x_test)\n",
    "        print(player,category)\n",
    "        print(r2_score(y_true=y_test,y_pred=y_pred))\n",
    "\n",
    "        saved_results[player][category]['linear_model']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n",
    "        saved_models[player][category]['linear_model'] = linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing for multicollinearity\n",
    "mulitcol_pairs = {cat:[] for cat in features.keys()}\n",
    "for category in features.keys():\n",
    "    corr_matrix = data_ordered[features[category]].corr()\n",
    "    high_corr_vars = np.where(abs(corr_matrix) > 0.8)\n",
    "    high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                    for x, y in zip(*high_corr_vars) if x != y and x < y]\n",
    "    mulitcol_pairs[category].append(high_corr_pairs)\n",
    "      # Drop one from each highly correlated pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_feats = {cat:[] for cat in features.keys()}\n",
    "for cat in mulitcol_pairs.keys():\n",
    "    print(cat)\n",
    "    for pairs in mulitcol_pairs[cat]:\n",
    "        for x,y in pairs:\n",
    "            cor_x = pearsonr(data_ordered[x],data_ordered[cat])\n",
    "            cor_y = pearsonr(data_ordered[y],data_ordered[cat])\n",
    "\n",
    "            if cor_x[1] < .05 and cor_x[0] > cor_y[0]:\n",
    "                trimmed_feats[cat].append(x)\n",
    "            elif cor_y[1] < .05 and cor_y[0] > cor_x[0]:\n",
    "                trimmed_feats[cat].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    features_list = [f for f in features[category] if f != category]\n",
    "    x_train,y_train = train_data[features_list],train_data[category]\n",
    "    x_test, y_test = test_data[features_list],test_data[category]\n",
    "    ridge_model = Ridge(alpha=1)\n",
    "\n",
    "    ridge_model.fit(x_train,y_train)\n",
    "\n",
    "    output = pd.DataFrame({'prediction':ridge_model.predict(x_test), 'actual':y_test})\n",
    "    print(category)\n",
    "    print(r2_score(y_true=output['actual'],y_pred=output['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data_ordered[numeric_columns])\n",
    "\n",
    "scaled_data_df = pd.DataFrame(scaled_data,columns=numeric_columns)\n",
    "\n",
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "scaled_train_data = scaled_data_df.iloc[:split_index]\n",
    "scaled_test_data = scaled_data_df[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,6,9],'learning_rate':[.01,.05,.1,.3],'booster':['gbtree','dart'],'subsample':[.5,.7,.9],'colsample_bytree':[.5,.7,.9],'n_estimators':[100,300,500]}\n",
    "param_linear = {'booster':['gblinear'],'lambda':[0,.1,1,10],'alpha':[0,.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = xgboost.XGBRegressor()\n",
    "mse_score = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "scoring = {'MSE':mse_score,'r2':r2_scorer}\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_grid,scoring = scoring,cv=tscv,n_jobs=1,verbose=0,refit='r2')\n",
    "grid_linear_search = GridSearchCV(estimator=xgb_regressor,param_grid=param_linear,scoring = scoring,cv=tscv,n_jobs=3,verbose=0,refit='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_features = [feature for feature in data_ordered.columns if data_ordered[feature].dtype == 'float' and feature not in features.keys() and feature != 'time_decay_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in features.keys():\n",
    "    x_train,y_train = scaled_train_data[xg_features],train_data[category]\n",
    "    x_test, y_test = scaled_test_data[xg_features],test_data[category]\n",
    "\n",
    "    fit_params = {'eval_set':[(x_test,y_test)],'early_stopping_rounds':20,'verbose':False}\n",
    "\n",
    "    grid_linear_search.estimator.set_params(eval_metric='rmse')\n",
    "\n",
    "\n",
    "    grid_linear_search.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    print(category)\n",
    "    print(grid_linear_search.best_params_)\n",
    "    print(grid_linear_search.best_score_)\n",
    "\n",
    "    y_pred = grid_linear_search.best_estimator_.predict(x_test)\n",
    "\n",
    "    saved_models[category]['XGboost'] = grid_linear_search.best_estimator_\n",
    "    saved_results[category]['XGboost']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = lightgbm.LGBMRegressor(boosting_type='gbdt', n_estimators=500)  # Using hist for faster training\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50],  \n",
    "    'learning_rate': [0.01, 0.1],  \n",
    "    'max_depth': [-1, 10],  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_grid_search = GridSearchCV(estimator=light,param_grid=param_grid,cv=tscv,verbose=0,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(data_ordered) * .80)\n",
    "\n",
    "train_data = data_ordered.iloc[:split_index]\n",
    "test_data = data_ordered[split_index:]\n",
    "for category in features.keys():\n",
    "    x_train,y_train = train_data[features[category]],train_data[category]\n",
    "    x_test,y_test = test_data[features[category]],test_data[category]\n",
    "\n",
    "    light_grid_search.fit(x_train,y_train)\n",
    "\n",
    "    best_model = light_grid_search.best_estimator_\n",
    "    print(category)\n",
    "    print(\"Best Parameters:\", light_grid_search.best_params_)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test,y_pred)\n",
    "\n",
    "    saved_models[category]['lightgbm'] = best_model\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'R2: {r2}')\n",
    "\n",
    "    saved_results[category]['lightgbm']={'r2':{r2_score(y_true=y_test,y_pred=y_pred)}, 'mse':{mean_squared_error(y_true=y_test,y_pred=y_pred)}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(saved_models,'models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_performance.txt', 'w') as file:\n",
    "    for category, models in saved_results.items():\n",
    "        file.write(f\"Category: {category}\\n\")\n",
    "        for model, metrics in models.items():\n",
    "            file.write(f\"  Model: {model}\\n\")\n",
    "            for metric, value in metrics.items():\n",
    "                file.write(f\"    {metric}: {value}\\n\")\n",
    "        file.write(\"\\n\")  # Newline between categories\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
